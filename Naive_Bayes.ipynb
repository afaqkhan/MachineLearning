{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4"
    },
    "colab": {
      "name": "Naive Bayes.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/afaqkhan/MachineLearning/blob/master/Naive_Bayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "BWXFxO_JC_rm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "77c09ce3-1502-4a17-8314-b9f311e3cd29"
      },
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "data = fetch_20newsgroups()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76P2ScOKC_r1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkVkmwWNC_sA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "60a2f209-e603-418e-ed5d-0fe76647acf1"
      },
      "source": [
        "categories = []\n",
        "categories = data.target_names\n",
        "print(categories)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZ8hJLnyC_sM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = fetch_20newsgroups(subset='train', categories = categories)\n",
        "test = fetch_20newsgroups(subset = 'test', categories = categories)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMiu5DceC_sV",
        "colab_type": "code",
        "colab": {},
        "outputId": "9ccb0e2a-e7e4-442d-b704-c909a0e2f55f"
      },
      "source": [
        "print(train.data[2])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "From: twillis@ec.ecn.purdue.edu (Thomas E Willis)\n",
            "Subject: PB questions...\n",
            "Organization: Purdue University Engineering Computer Network\n",
            "Distribution: usa\n",
            "Lines: 36\n",
            "\n",
            "well folks, my mac plus finally gave up the ghost this weekend after\n",
            "starting life as a 512k way back in 1985.  sooo, i'm in the market for a\n",
            "new machine a bit sooner than i intended to be...\n",
            "\n",
            "i'm looking into picking up a powerbook 160 or maybe 180 and have a bunch\n",
            "of questions that (hopefully) somebody can answer:\n",
            "\n",
            "* does anybody know any dirt on when the next round of powerbook\n",
            "introductions are expected?  i'd heard the 185c was supposed to make an\n",
            "appearence \"this summer\" but haven't heard anymore on it - and since i\n",
            "don't have access to macleak, i was wondering if anybody out there had\n",
            "more info...\n",
            "\n",
            "* has anybody heard rumors about price drops to the powerbook line like the\n",
            "ones the duo's just went through recently?\n",
            "\n",
            "* what's the impression of the display on the 180?  i could probably swing\n",
            "a 180 if i got the 80Mb disk rather than the 120, but i don't really have\n",
            "a feel for how much \"better\" the display is (yea, it looks great in the\n",
            "store, but is that all \"wow\" or is it really that good?).  could i solicit\n",
            "some opinions of people who use the 160 and 180 day-to-day on if its worth\n",
            "taking the disk size and money hit to get the active display?  (i realize\n",
            "this is a real subjective question, but i've only played around with the\n",
            "machines in a computer store breifly and figured the opinions of somebody\n",
            "who actually uses the machine daily might prove helpful).\n",
            "\n",
            "* how well does hellcats perform?  ;)\n",
            "\n",
            "thanks a bunch in advance for any info - if you could email, i'll post a\n",
            "summary (news reading time is at a premium with finals just around the\n",
            "corner... :( )\n",
            "--\n",
            "Tom Willis  \\  twillis@ecn.purdue.edu    \\    Purdue Electrical Engineering\n",
            "---------------------------------------------------------------------------\n",
            "\"Convictions are more dangerous enemies of truth than lies.\"  - F. W.\n",
            "Nietzsche\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHqReAtBC_sf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import make_pipeline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCBIsAyMC_sp",
        "colab_type": "code",
        "colab": {},
        "outputId": "7ff3f548-43c4-4bc5-fed9-9856cb343e21"
      },
      "source": [
        "model = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
        "model.fit(train.data, train.target)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('tfidfvectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
              "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
              "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
              "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_i...   vocabulary=None)), ('multinomialnb', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7z-9R5IC_s1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = model.predict(test.data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRL4EzxfC_tB",
        "colab_type": "code",
        "colab": {},
        "outputId": "d2462fee-924f-4cfe-f18c-82e3c7c3965f"
      },
      "source": [
        "pred = model.predict(['God'])\n",
        "print(pred)\n",
        "print(pred[0])\n",
        "train.target_names[pred[0]]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[15]\n",
            "15\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'soc.religion.christian'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y85cv_hpC_tO",
        "colab_type": "code",
        "colab": {},
        "outputId": "31b8f520-7cc9-4612-c99f-44f5db76f1bb"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "corpus = [\n",
        "'This is the first document.',\n",
        "'This document is the second document.',\n",
        "'And this is the third one.',\n",
        "'Is this the first document?']\n",
        "vectorizer = TfidfVectorizer()\n",
        "vect = vectorizer.fit_transform(corpus)\n",
        "print(vectorizer.get_feature_names())\n",
        "vect.toarray()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.        ,  0.46979139,  0.58028582,  0.38408524,  0.        ,\n",
              "         0.        ,  0.38408524,  0.        ,  0.38408524],\n",
              "       [ 0.        ,  0.6876236 ,  0.        ,  0.28108867,  0.        ,\n",
              "         0.53864762,  0.28108867,  0.        ,  0.28108867],\n",
              "       [ 0.51184851,  0.        ,  0.        ,  0.26710379,  0.51184851,\n",
              "         0.        ,  0.26710379,  0.51184851,  0.26710379],\n",
              "       [ 0.        ,  0.46979139,  0.58028582,  0.38408524,  0.        ,\n",
              "         0.        ,  0.38408524,  0.        ,  0.38408524]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsIPqrf5C_te",
        "colab_type": "code",
        "colab": {},
        "outputId": "20ba2fa4-0cef-439b-afe8-2d59d32003d8"
      },
      "source": [
        "import numpy as np\n",
        "X = np.random.randint(5, size=(6, 100))    #generates a 6x100 array containing values 0 to 4\n",
        "print(X)\n",
        "y = np.array([6, 4, 2, 5, 1, 3])\n",
        "print(y)\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "clf = MultinomialNB()\n",
        "clf.fit(X, y)\n",
        "print(X[2:3])\n",
        "print(clf.predict(X[2:3]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[4 0 4 4 1 1 3 1 2 3 2 0 1 0 3 2 2 3 3 1 4 1 3 4 2 2 3 2 1 0 1 3 4 3 0 1 1\n",
            "  2 3 2 2 3 4 3 0 0 4 1 3 4 1 2 3 1 2 1 4 4 2 4 2 1 0 4 0 1 4 2 3 2 1 2 2 3\n",
            "  4 4 1 1 3 0 1 1 2 3 4 4 0 3 4 1 1 1 0 3 2 2 4 2 3 1]\n",
            " [2 3 0 1 0 4 4 0 2 2 3 1 1 0 3 4 1 0 3 0 0 2 4 1 3 3 0 1 3 4 2 1 0 0 2 3 4\n",
            "  4 4 0 2 3 4 2 2 0 0 1 4 1 0 1 1 0 1 0 3 1 1 3 3 2 1 1 0 4 4 1 0 1 3 3 0 4\n",
            "  3 1 0 4 1 2 3 2 4 4 1 0 3 4 4 4 4 1 3 4 4 0 3 0 3 3]\n",
            " [4 4 1 0 2 2 1 0 3 0 1 1 1 1 2 4 2 4 3 4 0 0 2 4 0 1 3 3 1 2 0 1 2 2 3 0 2\n",
            "  1 0 3 0 3 2 2 3 3 0 2 3 3 1 4 3 1 2 1 3 1 3 3 4 2 3 3 0 2 4 4 3 2 2 2 4 3\n",
            "  4 2 2 4 0 3 0 3 0 2 4 1 0 2 2 3 1 2 1 0 2 2 1 2 0 2]\n",
            " [1 2 2 1 3 4 3 1 4 2 3 0 1 3 2 2 2 2 1 1 3 3 0 4 3 1 3 0 4 4 0 0 4 1 1 1 1\n",
            "  3 2 0 4 1 3 3 1 4 0 2 0 1 1 4 3 0 2 3 1 2 4 2 0 0 3 2 0 4 2 2 2 2 1 3 0 3\n",
            "  4 0 4 1 1 2 3 0 3 4 1 0 1 3 1 3 4 2 3 2 0 3 1 4 1 1]\n",
            " [1 1 4 2 4 1 0 1 4 2 3 0 4 0 1 1 0 0 3 2 2 0 4 3 3 2 3 2 3 1 1 4 1 3 2 0 4\n",
            "  4 3 3 1 0 3 1 1 2 1 3 2 3 1 4 3 1 2 4 3 2 4 1 1 0 4 4 2 3 3 2 2 4 0 4 1 1\n",
            "  1 3 0 2 4 0 0 1 2 4 1 1 4 2 2 0 0 3 4 0 3 3 2 1 4 4]\n",
            " [1 2 3 1 3 4 3 0 0 2 1 0 4 2 4 0 0 4 3 1 4 0 1 0 3 1 1 2 3 4 2 1 2 3 1 1 3\n",
            "  4 4 0 2 1 2 3 2 1 0 2 1 4 0 1 0 2 3 0 2 3 1 3 3 2 2 4 1 4 1 4 1 2 0 3 0 2\n",
            "  0 0 0 0 0 1 0 3 4 3 3 4 4 2 1 0 2 0 1 0 1 2 0 3 2 0]]\n",
            "[6 4 2 5 1 3]\n",
            "[[4 4 1 0 2 2 1 0 3 0 1 1 1 1 2 4 2 4 3 4 0 0 2 4 0 1 3 3 1 2 0 1 2 2 3 0 2\n",
            "  1 0 3 0 3 2 2 3 3 0 2 3 3 1 4 3 1 2 1 3 1 3 3 4 2 3 3 0 2 4 4 3 2 2 2 4 3\n",
            "  4 2 2 4 0 3 0 3 0 2 4 1 0 2 2 3 1 2 1 0 2 2 1 2 0 2]]\n",
            "[2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOeWSLdnC_tp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}